# ğŸ¾ Chatbot "Carla" usando Hugging Face - Blenderbot
# Modelo: facebook/blenderbot-400M-distill

from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration
import torch

print("ğŸ¾ Iniciando Carla...")

# Intentar cargar modelo y tokenizer desde Hugging Face
try:
    model_name = "facebook/blenderbot-400M-distill"
    tokenizer = BlenderbotTokenizer.from_pretrained(model_name)
    model = BlenderbotForConditionalGeneration.from_pretrained(model_name)
    print("âœ… Modelo cargado correctamente desde Hugging Face.")
except Exception as e:
    print(f"âš ï¸ Error al conectar con Hugging Face: {e}")
    exit()

# ğŸ’¬ FunciÃ³n para conversar
def chat_with_carla():
    print("\nğŸ¾ Carla: Â¡Hola! Soy tu asistente virtual. Escribe 'salir' para terminar.\n")

    while True:
        user_input = input("TÃº: ")
        if user_input.lower() in ["salir", "exit", "quit"]:
            print("ğŸ¾ Carla: Â¡Hasta luego! ğŸ•")
            break

        try:
            inputs = tokenizer([user_input], return_tensors="pt")
            reply_ids = model.generate(**inputs)
            reply = tokenizer.decode(reply_ids[0], skip_special_tokens=True)
            print(f"ğŸ¾ Carla: {reply}\n")
        except Exception as e:
            print(f"âš ï¸ OcurriÃ³ un error procesando tu mensaje: {e}\n")

# ğŸš€ Iniciar el chat
if __name__ == "__main__":
    chat_with_carla()
