import streamlit as st
from transformers import pipeline
from googletrans import Translator

# -----------------------------------------------------------
# CONFIGURACI√ìN DE LA P√ÅGINA
# -----------------------------------------------------------
st.set_page_config(
    page_title="CarlaTL - Asistente de Zoonosis",
    page_icon="üêæ",
    layout="centered"
)

st.title("üêæ Carla - Asistente Virtual de Zoonosis")
st.markdown(
    "¬°Hola! Soy **Carla**, tu asistente virtual. üê∂üê±<br>"
    "Puedo ayudarte con informaci√≥n sobre **zoonosis, vacunaci√≥n, prevenci√≥n y cuidado animal**.",
    unsafe_allow_html=True
)

# -----------------------------------------------------------
# CARGAR MODELO (TinyLlama)
# -----------------------------------------------------------
@st.cache_resource
def cargar_modelo():
    try:
        model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
        st.info(f"Cargando modelo `{model_name}`... Puede tardar unos segundos ‚è≥")
        model = pipeline("text-generation", model=model_name)
        st.success("‚úÖ Modelo cargado correctamente.")
        return model
    except Exception as e:
        st.error(f"‚ùå Error al cargar el modelo: {e}")
        return None

nlp = cargar_modelo()
translator = Translator()

# -----------------------------------------------------------
# CONTEXTO DEL CHATBOT
# -----------------------------------------------------------
contexto = (
    "Eres Carla, una asistente virtual especializada en zoonosis, vacunaci√≥n y cuidado animal. "
    "Brindas informaci√≥n confiable y clara sobre prevenci√≥n de enfermedades, campa√±as de vacunaci√≥n, "
    "cuidados veterinarios y tenencia responsable de mascotas. Respondes siempre en espa√±ol y con un tono amable."
)

if "historial" not in st.session_state:
    st.session_state.historial = ""

# -----------------------------------------------------------
# FUNCI√ìN DE RESPUESTA
# -----------------------------------------------------------
def responder(texto_es):
    if not texto_es.strip():
        return "Por favor, escrib√≠ una pregunta o mensaje."

    # Traducir al ingl√©s (TinyLlama fue entrenado principalmente en ingl√©s)
    texto_en = translator.translate(texto_es, src='es', dest='en').text

    prompt_en = (
        f"{contexto}\n\n"
        f"Previous conversation:\n{st.session_state.historial}\n\n"
        f"User: {texto_en}\nAssistant:"
    )

    try:
        generacion = nlp(
            prompt_en,
            max_new_tokens=60,
            do_sample=True,
            temperature=0.8,
            top_p=0.9,
            num_return_sequences=1
        )[0]
        respuesta_en = generacion['generated_text'][len(prompt_en):].strip()
    except Exception as e:
        respuesta_en = "I'm not sure how to respond to that."
        st.error(f"‚ö†Ô∏è Error interno del modelo: {e}")

    # Traducir respuesta al espa√±ol
    respuesta_es = translator.translate(respuesta_en, src='en', dest='es').text

    # Actualizar historial
    st.session_state.historial += f"\nUsuario: {texto_es}\nCarla: {respuesta_es}"
    return respuesta_es

# -----------------------------------------------------------
# INTERFAZ DE CHAT
# -----------------------------------------------------------
user_input = st.text_input("üí¨ Escrib√≠ tu consulta aqu√≠:")

if st.button("Enviar"):
    if nlp:
        respuesta = responder(user_input)
        st.markdown(f"**üêæ Carla:** {respuesta}")
    else:
        st.error("El modelo no se pudo cargar correctamente.")

# Mostrar historial opcional
with st.expander("üß† Ver historial de conversaci√≥n"):
    st.text(st.session_state.historial)
